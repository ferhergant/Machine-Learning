{"cells":[{"cell_type":"markdown","source":["##  Gaussian Mixture Models (GMM) in Spark\n\n------------------------------------------------------\n*Machine Learning, Master in Big Data Analytics, 2017-2018*\n\n*Vanessa Gómez Verdejo vanessa@tsc.uc3m.es*\n\n------------------------------------------------------"],"metadata":{}},{"cell_type":"markdown","source":["A [**Gaussian Mixture Models** (GMM)](https://brilliant.org/wiki/gaussian-mixture-model/) represents a composite distribution whereby points are drawn from one of K Gaussian sub-distributions. The probabilistic distribution of a data **x** is given by:\n\n$$ p(\\mathbf{x}) = \\sum_k^K \\pi_k \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k) $$\n\n$$ {\\rm ~where~} \\pi_k {\\rm ~are~ the ~mixing ~~weights},~ \\pi_k>0,~ \\sum_k \\pi_k =1.$$\n$$\\mathbf{\\mu}_k {\\rm ~and~} \\mathbf{\\Sigma}_k {\\rm ~are ~the ~mean~and ~covariance ~matrix~of ~the ~k-th ~gaussian ~distribution}.$$\n\nThe GMM is one of the most common of probabilistic generative models and it can be used for both density estimation and clustering. Fitting the parameters of Mixture Models is usually carried out by means of the [**Expectation-Maximization (EM)**](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) algorithm which provides a simple iterative solution that guarantees to converge to a local mode.\n\nIn this notebook, let's work with this model over Spark. In particular, we will review the following concepts:\n\n* **1. Introduction to GMM with MLLIB:**\n  - Data generation\n  - Learning a GMM with MLLib\n  - Estimating the data likelihood\n* **2. Model application: novelty detection**\n* **3. Implementing the EM algorithm in Spark**"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Introduction to GMM with MLLib"],"metadata":{}},{"cell_type":"markdown","source":["#### 1.1 Data generation\n\nTo start to work with the GMM model, the next cell generates a toy dataset of samples coming from a GMM and creates a RDD to work with this dataset over Spark.\n\nNote: The GMM implementation of Spark only works with RDDs, so throughout this practice we will use this type of data instead of DataFrames."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\n\n# Set a sedd to get similar results\nnp.random.seed(4)\n\n# Define the number of samples\nn_samples = 5000\n\n# Let's create a bidimensional model with 2 gaussians. Define their means and covariance matrix\nmean0 = [-2, -2]\nmean1 = [2, 2]\nsigma = np.eye(2)\n\ndata0  = np.random.multivariate_normal(mean0, sigma, n_samples/2)\ndata1  = np.random.multivariate_normal(mean1, sigma, n_samples/2)\n\ndata = np.vstack((data0,data1))\n#Paralelizamos los datos\ndataRDD = sc.parallelize(data)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["**Exercise**: Complete the following code to provide the desired output."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\n# Compute the number of data in the RDD\nnumberData = dataRDD.count()\nprint \"Total number of data:\" , numberData\n\n# Get the first 5 data of the RDD\ndata5 = dataRDD.take(5)\nprint \"The first 5 data are:\", data5"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nTotal number of data: 5000\nThe first 5 data are: [array([-1.94943829, -1.50004867]), array([-2.99590893, -1.30640149]), array([-2.41830152, -3.58457724]), array([-2.64770677, -1.40142483]), array([-1.66774997, -3.14747663])]\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["**Exercise**: To analyze the dataset in detail, the next cell gives you some lines of code to plot the data. Since we are supposed to be working with a large amount of data, we cannot collect the complete dataset to the driver, so let's randomly take 500 data and plot this subset in a two-dimensional space.\n\nNote: Use the RDD method takeSample()."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\nimport matplotlib.pyplot as plt\n\n# Take (at random) 500 data from dataRDD\ndata500 = dataRDD.takeSample(False,500)\n\n# Note that you are getting a list of arrays, convert it to ndnumpy array (use np.array() method)\ndata500 = np.array(data500)\n\n# Plot the data\nfig = plt.figure()\nplt.plot(data500[:,0],data500[:,1],'.')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\n<img src=\"http://www.tsc.uc3m.es/~vanessa/figsdatabricks/GMM0.png\" width=\"400\"/>\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["#### 1.2 Learning a GMM with MLLib\n\nNow, let's learn the underlying probability distribution of this dataset. For this purpose, let's use the [GaussianMixture object of MLLIB](https://spark.apache.org/docs/2.2.0/mllib-clustering.html#gaussian-mixture).\n\nThis model can be called as:\n\ngmmModel = GaussianMixture.train(rdd, k, convergenceTol=0.001, maxIterations=100, seed=None, initialModel=None)\n\nwhere:\n* rdd – Training points as an RDD of Vector or convertible sequence types.\n* k – Number of independent Gaussians in the mixture model.\n* convergenceTol – Maximum change in log-likelihood at which convergence is considered to have occurred. (default: 1e-3)\n* maxIterations – Maximum number of iterations allowed. (default: 100)\n* seed – Random seed for initial Gaussian distribution. Set as None to generate seed based on system time. (default: None)\n* initialModel – Initial GMM starting point, bypassing the random initialization. (default: None)\n\n\n**Exercise**: Train a GMM for the dataRDD generated before, fixing k=2  and setting seed=1 (use the default values for the reamining parameters).\n\nNote: This cell only trains the model and no output is generated."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\nfrom pyspark.mllib.clustering import GaussianMixture, GaussianMixtureModel\n\ngmmModel = GaussianMixture.train(dataRDD, 2, convergenceTol=0.001, maxIterations=100, seed=None, initialModel=None)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["**Analyze the output of the model**\n\nThe resulting [gmmModel](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.clustering.GaussianMixtureModel) has the following parameters:\n* .k: Number of gaussians in mixture.\n* .weights: Weights for each gaussian distribution in the mixture, where weights[i] is the weight for Gaussian i, and weights.sum == 1.\n* .gaussian: Array of MultivariateGaussian where gaussians[i] represents the Multivariate Gaussian (Normal) Distribution for the i-th Gaussian. Each element of the array, gaussians[i], has two parameters:\n  * .mu: mean of the gaussian distribution\n  * .sigma: covariance matrix of the gaussian distribution\n  \n**Exercise**: Use this information to complete the following cell and analyze the learned model. \nCheck if the parameters learned by the model match with those used to generate the data."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\n# Check output parameters of model\n# Get the number of gaussians in the model\nK= gmmModel.k\nprint \"Number of gaussians \", K\nprint \"----------\"\n\n# For each gaussian\nfor i in range(K):\n    print \"Gaussian \", i\n    # Get the weight associated to each Gaussian distribution in the mixture\n    w = gmmModel.weights[i] \n    print \"weight = \", w\n    # Get the mean of the i-th Gaussian\n    mu = gmmModel.gaussians[i].mu\n    print \"mu = \", mu\n    # Get the covariance matrix of the i-th Gaussian\n    sigma = gmmModel.gaussians[i].sigma\n    print \"sigma = \", sigma.toArray()\n    print \"----------\""],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nNumber of gaussians  2\n----------\nGaussian  0\nweight =  0.499588708829\nmu =  [1.99809478028,1.99804110237]\nsigma =  [[ 0.99857662  0.00233737]\n [ 0.00233737  1.04104924]]\n----------\nGaussian  1\nweight =  0.500411291171\nmu =  [-1.99951629153,-1.98433624412]\nsigma =  [[ 0.94428854 -0.01510081]\n [-0.01510081  0.97834726]]\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["**Compute the data membership**\n\nThe gmmModel also has two additional methods:\n* .predict: to find the cluster to which a new point (or for each point in an RDD) has maximum membership in this model.\n* .predictSoft: to compute the membership of a new point (or for each point in an RDD) to all mixture components. \n\n**Exercise**:  The next cell provides you 4 example data points, compute both the membership of each data point to each gaussian and the maximum membership of the model."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\n# Create an RDD with 4 example data points\ntestDataRDD = sc.parallelize([np.array([ 1.50397636,  1.83196124]),\n np.array([-2.18957109,  -2.30737699]),\n np.array([-10.0,  10.0]),\n np.array([0.0 ,  0.0])])\n\n# Compute the maximum membership of each data point\nmaxMembership= gmmModel.predict(testDataRDD)\nprint maxMembership.collect()\n\n# Compute the membership of each data point to each gaussian \nsoftMembership= gmmModel.predictSoft(testDataRDD)\nprint softMembership.collect()\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\n[0, 1, 0, 0]\n[array('d', [0.9999991418624413, 8.581375586904735e-07]), array('d', [2.2038433622319005e-08, 0.9999999779615664]), array('d', [0.5, 0.5]), array('d', [0.5565069464261408, 0.4434930535738591])]\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["**Note** that among the 4 example data, the first two could have been generated by the original model, but it is unlikely that the last two were generated by the same distribution that generated dataRDD. However, the functions .predict() and .Softpredict() assign them to one of the Gaussians and they do not indicate if they belong to it a high or low probability. \n\nThis is due to the fact that the MLLIB GMM model is designed for clustering purposes (it assigns a data to the most likely cluster) and it is not intended for probability estimation (i.e., providing the data probability under this model). In the next section, we will design the necessary functions to give the model this additional functionality."],"metadata":{}},{"cell_type":"markdown","source":["#### 1.3 Estimating the model likelihood\n\nLet's consider that a GMM model is given (or we have already learned it from a given data set), so the following set of parameters is known:\n\n$$\\mathbf{\\theta}\\triangleq[\\pi_1, \\ldots,\\pi_K,\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_K,\\mathbf{\\Sigma}_1,\\ldots,\\mathbf{\\Sigma}_K]$$\n\nand our goal is to know with what probability this model has generated a sample data point **x**, that is, we want to compute:\n\n$$ p(\\mathbf{x}|\\mathbf{\\theta}) = \\sum_k^K \\pi_k \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k)$$\n\nthis is known as the model likelihood for the data **x**.\n\nIn this section we are going to implement some useful functions that will allow us to calculate the model likelihood of a given set of data."],"metadata":{}},{"cell_type":"markdown","source":["**Exercise**: Since the model likelihood for a given data **x** is given by the sum of a set of K Gaussian distributions, let's start by desingning a function, computeLikelihoodPerGauss(), which calculates the K probabilities of the data **x** on each of these Gaussian distributions, i.e.,\n\n$$ [ \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_1,\\mathbf{\\Sigma}_1), \\ldots,  \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_K,\\mathbf{\\Sigma}_K)]$$\n\nThe next cell gives you some code lines to calculate with what probability an example data has been generated by the first gaussian:\n\n$$ p_1(\\mathbf{x}|\\mathbf{\\theta}) = \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_1,\\mathbf{\\Sigma}_1)$$\n\nUse this sample code to complete the function computeLikelihoodPerGauss()."],"metadata":{}},{"cell_type":"code","source":["# SAMPLE CODE: It computes the likelihood of the first gaussian of model over an example data\n\nfrom scipy.stats import multivariate_normal\n\n# Example data: You can try to change the data values to obtain different probabilities values\ndata=np.array([ -2,  -2])\n# Get the model parameters (we are using the model trained in the previous section)\nmu = gmmModel.gaussians[0].mu\nsigma = gmmModel.gaussians[0].sigma.toArray()\n\nprint 'Model parameters: ', mu, sigma\n# Define the gaussian distribution (with the previous parameters) and compute its probability over data\ngauss= multivariate_normal(mu, sigma)\nlikelihood = gauss.pdf(data)\n\n# Print the output\nprint 'Gaussian_0 likelihood for data:', likelihood"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\ndef computeLikelihoodPerGauss(gaussians, x):\n  \n    \"\"\"Compute the likelihood of each gaussian for the data x\n    Args:\n      gaussians: list with the gaussians of the GMM. Each gaussian is an object with the mean and coviariance of the gaussian.\n      x: numpy array with a data\n    Returns: \n      vectorLikelihoods: numpy array with the likelihood of data over each gaussian of the GMM model\n    \"\"\"\n    likelihood = np.zeros(len(gaussians))\n    \n    for i in range(len(gaussians)):\n      \n      mu = gaussians[i].mu\n      sigma = gaussians[i].sigma.toArray()\n      gauss= multivariate_normal(mu, sigma)\n      likelihood[i] = gauss.pdf(x)\n    \n    return likelihood\n\n\n# 1. Check the function over a data\ndata=np.array([ -1.5,  -0.5])\n# Use the gaussian of the previous GMM model\ngaussians = gmmModel.gaussians\n# Compute the gaussians' likelihoods\nprint computeLikelihoodPerGauss(gaussians, data)\n\n\n# 2. Check the function over an RDD\ntestDataRDD = sc.parallelize([np.array([ 1.5,  1.8]),\n np.array([-2.1,  -2.3]),\n np.array([-10.0,  10.0]),\n np.array([0.0 ,  0.0])])\n\nRDDlikelihoods = testDataRDD.map(lambda x: computeLikelihoodPerGauss(gaussians, x)) \nprint RDDlikelihoods.collect()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nGaussians likelihoods over a data:  [  1.73518956e-05   4.64812309e-02]\nGaussians likelihoods over RDD:  [array([  1.35319224e-01,   1.34496203e-07]), array([  5.07317642e-09,   1.56460786e-01]), array([  2.75170192e-46,   1.95606177e-47]), array([ 0.00313623,  0.00249522])]\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["**Exercise**: Using the function computeLikelihoodPerGaussian (), complete the code of the function computeGMMLikelihood() so that it calculates the likelihood of the GMM which is given by the weigthed sum of the gaussians' likehoods:\n$$ p(\\mathbf{x}|\\mathbf{\\theta}) = \\sum_k^K \\pi_k \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k)$$"],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\ndef computeGMMLikelihood(gaussians, weights, x):\n  \n    \"\"\"Compute the likelihood of the GMM model for the data x\n    Args:\n      gaussians: list with the gaussians of the GMM. Each gaussian is an object with the mean and coviariance of the gaussian.\n      weights: weights for each Gaussian distribution in the GMM \n      x: numpy array with a data\n    Returns: \n      Likelihood: likelihood of the GMM model over data\n    \"\"\"\n    gaussian_likelihood = computeLikelihoodPerGauss(gaussians,x)   \n    \n    return np.sum(np.multiply(weights,gaussian_likelihood))\n  \n\n# 1. Check the function over a data\ndata=np.array([ -1.5,  -0.5])\n# Use the gaussian of the previous GMM model\ngaussians = gmmModel.gaussians\nweights = gmmModel.weights\n# Compute the gaussians' likelihoods\nlikelihoodData =  computeGMMLikelihood(gaussians, weights, data)\nprint 'GMM likelihood over a data: ', likelihoodData\n\n\n# 2. Check the function over an RDD\ntestDataRDD = sc.parallelize([np.array([ 1.5,  1.8]),\n np.array([-2.1,  -2.3]),\n np.array([-10.0,  10.0]),\n np.array([0.0 ,  0.0])])\n\nRDDlikelihoods = testDataRDD.map(lambda x: computeGMMLikelihood(gaussians, weights, x))\nlikelihoodRDDData = RDDlikelihoods.collect()\nprint 'GMM likelihood over RDD: ', likelihoodRDDData"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nGMM likelihood over a data:  0.0232684015601\nGMM likelihood over RDD:  [0.067604023505349914, 0.078294746240264668, 1.4726027464313521e-46, 0.0028154602232752829]\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["In some cases, calculating the likelihood values can provide numerical problems. Keep in mind that some data have a very low values and others have large values, so numerical problems may arise. To avoid this, it is quite common to calculate the log-likelihood values instead of the likelihood ones.\n\n**Exercise**: Modify the previous functions to compute:\n* The log-likelihood of each gaussian for a given data -> computeLogLikelihoodPerGaussian()\n* The log-likelihood of the GMM model for a given data -> computeGMMLogLikelihood()\n\nYou may find useful the following functions:\n* [logpdf( )](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html)  of the multivariate_normal distribution: computes  the log of the multivariate_normal probability density function.\n* [logsumexp( )](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.misc.logsumexp.html): computes the log of the sum of exponentials of input elements."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\ndef computeLogLikelihoodPerGaussian(gaussians, x):\n  \n    \"\"\"Compute the log-likelihood of each gaussian for the data x\n    Args:\n      gaussians: list with the gaussians of the GMM. Each gaussian is an object with the mean and coviariance of the gaussian.\n      x: numpy array with a data\n    Returns: \n      vectorLogLikelihoods: numpy array with the log-likelihood of data over each gaussian of the GMM model\n    \"\"\"\n    likelihood = np.zeros(len(gaussians))\n    \n    for i in range(len(gaussians)):\n      \n      mu = gaussians[i].mu\n      sigma = gaussians[i].sigma.toArray()\n      gauss= multivariate_normal(mu, sigma)\n      likelihood[i] = gauss.logpdf(x)\n    \n    return likelihood\n\n# 1. Check the function over a data\ndata=np.array([ -1.5,  -0.5])\n# Use the gaussian of the previous GMM model\ngaussians = gmmModel.gaussians\n# Compute the gaussians' log-likelihoods\nlogLikelihoodData =  computeLogLikelihoodPerGaussian(gaussians, data)\nprint 'Gaussians log-likelihoods over a data: ', logLikelihoodData\n\n\n# 2. Check the function over an RDD\ntestDataRDD = sc.parallelize([np.array([ 1.5,  1.8]),\n np.array([-2.1,  -2.3]),\n np.array([-10.0,  10.0]),\n np.array([0.0 ,  0.0])])\n\nRDDlogLikelihoods = testDataRDD.map(lambda x: computeLogLikelihoodPerGaussian(gaussians, x)) \nlogLikelihoodRDDData = RDDlogLikelihoods.collect()\nprint 'Gaussians log-likelihoods over RDD: ', logLikelihoodRDDData"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nGaussians log-likelihoods over a data:  [-10.9618088   -3.06870669]\nGaussians log-likelihoods over RDD:  [array([ -2.00011867, -15.82172987]), array([-19.0992987 ,  -1.85494987]), array([-104.90669468, -107.55056622]), array([-5.76473507, -5.99337774])]\n</code></pre>"],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\nfrom scipy.misc import logsumexp\n\ndef computeGMMLogLikelihood(gaussians, weights, x):\n  \n    \"\"\"Compute the log-likelihood of the GMM model for the data x\n    Args:\n      gaussians: list with the gaussians of the GMM. Each gaussian is an object with the mean and coviariance of the gaussian.\n      weights: weights for each Gaussian distribution in the GMM \n      x: numpy array with a data\n    Returns: \n      LogLikelihood: log-likelihood of the GMM model over data\n    \"\"\"  \n    gaussian_likelihood = computeLikelihoodPerGauss(gaussians,x)   \n    return np.log(np.sum(np.multiply(weights,gaussian_likelihood)))\n  \n\n# 1. Check the function over a data\ndata=np.array([ -1.5,  -0.5])\n# Use the gaussian of the previous GMM model\ngaussians = gmmModel.gaussians\nweights = gmmModel.weights\n# Compute the gaussians' likelihoods\nlogLikelihoodData =  computeGMMLogLikelihood(gaussians, weights, data)\nprint 'GMM log-likelihood over a data: ', logLikelihoodData\n\n\n# 2. Check the function over an RDD\ntestDataRDD = sc.parallelize([np.array([ 1.5,  1.8]),\n np.array([-2.1,  -2.3]),\n np.array([-10.0,  10.0]),\n np.array([0.0 ,  0.0])])\n\nRDDlogLikelihoods = testDataRDD.map(lambda x: computeGMMLogLikelihood(gaussians, weights, x))\nlogLikelihoodRDDData = RDDlogLikelihoods.collect()\nprint 'GMM log-likelihood over RDD: ', logLikelihoodRDDData"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nGMM log-likelihood over a data:  -3.76065899504\nGMM log-likelihood over RDD:  [-2.6940877783945125, -2.5472747760620225, -105.53188286675339, -5.8726295411352369]\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["Following code allows you plot the GMM negative log-likelihood over a bidimensional space"],"metadata":{}},{"cell_type":"code","source":["# display predicted scores by the model as a contour plot\nfrom matplotlib.colors import LogNorm\n\nx = np.linspace(-10., 10.,20)\ny = np.linspace(-10., 10.,20)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\n\ngaussians = gmmModel.gaussians\nweights= gmmModel.weights\nZ =np.array([-computeGMMLogLikelihood(gaussians, weights,xx) for xx in XX])\nZ = Z.reshape(X.shape)\n\nfig = plt.figure()\nCS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1, vmax=100.0),\n                 levels=np.logspace(0, 2, 20))\nCB = plt.colorbar(CS, shrink=0.8, extend='both')\n\nplt.title('Negative log-likelihood predicted by a GMM')\nplt.axis('tight')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## 2. Model application: novelty detection\n\nMany applications need to decide whether a sample belongs to the same distribution as the existing data or it must be considered different (it is an outlier). We can consider two scenarios:\n* Novelty detection: We learn a density model wiht the training data and we are interested in detecting anomalies in new observations.\n* Outlier detection: The training data contains outliers, and we must density distribution of the training data and detect the deviated observations, i.e., the data that are less likely to have been generated by the predominant distribution of training data. Often, this capability is used to clean real data sets.\n\nIn this section, we are going to aplly the GMM to learn the data distribution of a given data set and, later, clean this dataset by detecting possible outliers."],"metadata":{}},{"cell_type":"markdown","source":["##### 1. Load data\n\nIn this section, let's work with the dataset provided in the file \"outlier_data.txt\". To work with this file, we firstly have to upload the  file to Databricks:\n* Go to Data (on left bar menu) and press over the sign '+' of Tables (new table)\n* Select:\n  - Data source: 'Upload File' (default option)\n  - Upload to DBFS: don't do anything (we will use a default path)\n  - File: drop or select the file 'outlier_data.txt'\n\nThen, you will see \"File uploaded to /FileStore/tables/outlier_data.txt\". This indicates that the file has been sucessfully uploaded and it also provides you the path file (\"/FileStore/tables/outlier_data.txt\"). Now, go back to the notebook and run the following cell to create an RRD with the content of the text file. Note that the next cell is also parsing the data to create an array for each line of the file  and each one of these arrays becomes an element of your RDD."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\n\n# Load and parse the data\ndata = sc.textFile(\"/FileStore/tables/outlier_data.txt\")\ndataRDD = data.map(lambda line: np.array([float(x) for x in line.strip().split(' ')]))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["**Exercise**: Using the sample code of Section 1.1, randomly take 500 data from the RDD and plot this subset in a two-dimensional space."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\nimport matplotlib.pyplot as plt\n\n# Take (at random) 500 data from dataRDD\ndata500 = dataRDD.takeSample(False,500)\n\n# Note that you are getting a list of arrays, convert it to ndnumpy array (use np.array() method)\ndata500 = np.array(data500)\n\n# Plot the data\nfig = plt.figure()\nplt.plot(data500[:,0],data500[:,1],'.')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\n<img src=\"http://www.tsc.uc3m.es/~vanessa/figsdatabricks/GMM1.png\" width=\"400\"/>\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["Observing the data representation, how many Gaussians do you think you would need to adjust the distribution of the data?"],"metadata":{}},{"cell_type":"markdown","source":["##### 2. Learning GMM model \n\nNow, let's train a GMM to learn the underlying distribution of the data set.\n\n**Exercise**: Using the code of Section 1.2, train a GMM with dataRDD (fix K=2, set the seed to 1, and leave the rest of the parameters their default values). Examine the learned model parameters and discuss if their values match with the expected values."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\nfrom pyspark.mllib.clustering import GaussianMixture, GaussianMixtureModel\n\n# Train the GMM\ngmmModel = GaussianMixture.train(dataRDD, 2, convergenceTol=0.001, maxIterations=100, seed=1, initialModel=None)\n\n# Check output parameters of model\n# Get the number of gaussians in the model\nK= gmmModel.k\nprint \"Number of gaussians \", K\nprint \"----------\"\n\n# For each gaussian\nfor i in range(K):\n    print \"Gaussian \", i\n    # Get the weight associated to each Gaussian distribution in the mixture\n    w = gmmModel.weights[i]\n    print \"weight = \", w\n    # Get the mean of the i-th Gaussian\n    mu = gmmModel.gaussians[i].mu\n    print \"mu = \", mu\n    # Get the covariance matrix of the i-th Gaussian\n    sigma = gmmModel.gaussians[i].sigma\n    print \"sigma = \", sigma.toArray()\n    print \"----------\""],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nNumber of gaussians  2\n----------\nGaussian  0\nweight =  0.530419057329\nmu =  [-0.0972356111299,0.0309637487178]\nsigma =  [[ 5.01018772 -1.93116286]\n [-1.93116286  1.01552125]]\n----------\nGaussian  1\nweight =  0.469580942671\nmu =  [0.0800047791251,0.0140670070328]\nsigma =  [[ 4.82964561  1.90124327]\n [ 1.90124327  0.94018857]]\n----------\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["##### 3. Outlier detection\n\nNow let's compute the model log-likelihood over each data of the dataset and find the data with less probability of having been generated by the GMM model.\n\n**Exercise**: complete the following code, using the functions implemented in Section 1.3, to complete the GMM log-likelihood of each data in dataRDD and find the 10 data with lower log-likelihood (i.e., the 10 most likely data to be outliers).\n\nNote: you may need the method takeOrdered() of the RDD."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\n# Extract the gaussians and weigths of the GMM model\ngaussians = gmmModel.gaussians\nweights= gmmModel.weights\n\n# Compute the GMM log-likelihood over dataRDD. Create as output a new RDD with tuples (data, log-likelihood)\nRDDLogLikelihoods = dataRDD.map(lambda x: (x,computeGMMLogLikelihood(gaussians, weights, x)))\nprint 'Example of the new RDD', RDDLogLikelihoods.first()\n\n# Find the 10 data with lower log-likelihood\noutliers = RDDLogLikelihoods.takeOrdered(10, key = lambda x: x[1])\nprint 'The 10 outliers are:', outliers\n"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nExample of the new RDD (array([ 2.59470454,  2.12298217]), -6.4751181828718067)\nThe 10 outliers are: [(array([ 0.09408561, -5.38938137]), -55.339682686473225), (array([ 0.07475756, -3.72503752]), -27.731269318686529), (array([-6.9801447 ,  0.07165119]), -19.936121021327619), (array([-6.12758116,  0.02153393]), -16.294868999153188), (array([ 5.55820383, -0.00647992]), -14.257275898865327), (array([ 5.19871928,  0.00982429]), -12.898483199148366), (array([ 4.74016965, -0.04789392]), -10.80555188845196), (array([ 4.53040133,  0.01067131]), -10.385510132932316), (array([-7.75688143, -3.65219926]), -9.7945077935091636), (array([-6.92959466, -3.57682881]), -9.4450051927093988)]\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["##### 4. Plot the result\n\n**Exercise**: Finally, let's plot the result. So using the sample code of Sections 1.1 (to plot the training data) and 1.3 (to plot the GMM negative log-likelihood), plot into a single figure:\n* the GMM negative log-likelihood\n* 500 data (at random) from the training data (in blue color)\n* the detected outliers (in red color)"],"metadata":{}},{"cell_type":"code","source":["\nfig = plt.figure()\nplt.plot(data500[:,0],data500[:,1],'b.')\n\n# display predicted scores by the model as a contour plot\nfrom matplotlib.colors import LogNorm\n\nx = np.linspace(-10., 10.,20)\ny = np.linspace(-10., 10.,20)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\n\ngaussians = gmmModel.gaussians\nweights= gmmModel.weights\nZ =np.array([-computeGMMLogLikelihood(gaussians, weights,xx) for xx in XX])\nZ = Z.reshape(X.shape)\n\nCS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1, vmax=100.0),\n                 levels=np.logspace(0, 2, 20))\nCB = plt.colorbar(CS, shrink=0.8, extend='both')\n\ndata, _ = zip(*outliers)\ndata = np.array(data)\nplt.plot(data[:,0],data[:,1],'r*')\n\nplt.title('Outliers detection with GMM')\nplt.axis('tight')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\n<img src=\"http://www.tsc.uc3m.es/~vanessa/figsdatabricks/GMM2.png\" width=\"400\"/>\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["## 3. Implementing the EM algorithm in Spark"],"metadata":{}},{"cell_type":"markdown","source":["The [Expectation–Maximization (EM)](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) algorithm is an iterative method to find maximum likelihood estimates of parameters in statistical models. To apply this algorithm to the GMM, let's imagine we are given a dataset \n$$\\{\\mathbf{x}^{(i)}\\},~~ i=1,\\ldots,N,$$\nand we want to fit a GMM to explain this data set. So, for a given number K of components, our goal is to find the parameters $$\\mathbf{\\theta}\\triangleq[\\pi_1, \\ldots,\\pi_K,\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_K,\\mathbf{\\Sigma}_1,\\ldots,\\mathbf{\\Sigma}_K]$$\nto maximize the **log-likelihood of the observed data**:\n\n$$\n\\ell(\\mathbf{\\theta}) = \\sum_{i}^{N} \\log p(\\mathbf{x}^{(i)}|\\mathbf{\\theta}) \n$$\n\n$$\n\\ell(\\mathbf{\\theta}) = \\sum_{i}^{N} \\log \\left[  \\sum_k^K \\pi_k \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k) \\right]\n$$ \n\nUnfortunately this is hard to optimize, since the log cannot be pushed inside the sum. To solve this problem,  a latent variable **z** is introduced indicating that the data **x** has been generated by the k-th component or distribution. In this way, this latent variable has a 1-of-K representation in which the element k-th is equal to 1 and all other elements are equal to 0, and its marginal distribution is given by\n\n$$\np(\\mathbf{z}) = \\prod_k^K \\pi_k^{z_k}\n$$\n\nand, therefore, the conditional distribution of **x** given **z** can be written in the form\n\n$$\np(\\mathbf{x}| \\mathbf{z}) =  \\prod_k^K  \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k)^{z_k}\n$$\n\nand the marginal distribution of **x** can be recovered by summing the joint distribution of **x** and **z** over alll possible states of **z** to give a gaussian mixture:\n\n$$\np(\\mathbf{x}) = \\sum_z p(\\mathbf{x}| \\mathbf{z}) p(\\mathbf{z}) = \\sum_k^K \\pi_k \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k)\n$$\n\n\nTo compute the model parameters, the **Expectation Maximization (EM)** algorithm defines the **complete data log likelihood** as\n\n$$\\ell_c(\\mathbf{\\theta})=\\sum_i^{N} \\log  p(\\mathbf{x}^{(i)},z^{(i)}|\\mathbf{\\theta})$$\n\nand, as the latent variable **z** is unknown, the EM computes the expected value of the complete data log likelihood under the posterior distribution of the latent variables (this is the E-step) and it later maximizes this expected log-likelihood (M-step).\n\nThe steps of the EM algorithm are:\n1. Choose a initial setting for the parameters:\n  $$\\mathbf{\\theta}^{old}\\triangleq[\\pi^{old}_1, \\ldots,\\pi^{old}_K,\\mathbf{\\mu}^{old}_1,\\ldots,\\mathbf{\\mu}^{old}_K,\\mathbf{\\Sigma}^{old}_1,\\ldots,\\mathbf{\\Sigma}^{old}_K]$$\n\n2. **E-step**: Compute the posterior distribution of the latent variable using the current parameter values. These probabilities are known are the **responsabilities** :\n$$\\mathbf{\\gamma}(\\mathbf{z}^{(i)})=p(\\mathbf{z}^{(i)}|\\mathbf{x}^{(i)},\\mathbf{\\theta}^{old})$$\nand are given by:\n$$\\gamma(z^{(i)}_{k}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}^{(i)}|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k) }{\\sum_j^K \\pi_j \\mathcal{N}(\\mathbf{x}^{(i)}|\\mathbf{\\mu}_j,\\mathbf{\\Sigma}_j) }$$\nThis value can be viewed as the responsility the component *k* takes for explaining the observation **x**.\n3. **M-step**: Compute the expected value of the complete data log likelihood and maximize it to reestimate the new values of the parameters:\n$$ \\mathbf{\\mu}^{new}_k = \\frac{1}{N_k} \\sum_i^N  \\gamma(z^{(i)}_k) \\mathbf{x}^{(i)} $$\n$$ \\mathbf{\\Sigma}^{new}_k = \\frac{1}{N_k} \\sum_i^N  \\gamma(z^{(i)}_k) (\\mathbf{x}^{(i)} -\\mathbf{\\mu}^{new}_k)(\\mathbf{x}^{(i)} -\\mathbf{\\mu}^{new}_k)^T $$\n$$ \\pi^{new}_k = \\frac{N_k}{N} $$\nwhere\n$$ N_k = \\sum_i^N  \\gamma(z^{(i)}_k) $$\n\n4. Evaluate the log likelihood\n$$ \\sum_i^{N} \\log  p(\\mathbf{x}^{(i)}|\\mathbf{\\theta}) = \\sum_i^{N} \\log \\left( \\sum_k^K \\pi_k \\mathcal{N}(\\mathbf{x}^{(i)}|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k) \\right)$$\nand check the convergence of either the parameters or the log likelihood. If the convergence criterion is not satisfied return to Step 2."],"metadata":{}},{"cell_type":"markdown","source":["In this section we will implement the EM algorithm, following the steps described in the previous cell (Steps 1 to 4). In the following cells, you will have to complete several exercises to implement each of these steps. To check the method funcionality, we will run the algorithm over the toy problem that is created in the next cell."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport matplotlib.pyplot as plt\n\n## Toy problem generation\n\n# Set a seed to get similar results\nnp.random.seed(10)\n\n# Number of samples\nn_samples = 2000\n\n# We generate 4 bidimensional data subsets with different gaussian parameters\nK = 4\ndata0 = np.vstack((np.random.randn(n_samples/K) + 4,np.random.randn(n_samples/K) + 4)).T\ndata1 = np.vstack((np.random.randn(n_samples/K) - 4,np.random.randn(n_samples/K) + 4)).T\ndata2 = np.vstack((np.random.randn(n_samples/K) - 4,np.random.randn(n_samples/K) - 4)).T\ndata3 = np.vstack((np.random.randn(n_samples/K) + 4,np.random.randn(n_samples/K) - 4)).T\n# Combine all the subsets\ndata = np.vstack((data0,data1,data2,data3))\n\n# Parallelize the data\ndataRDD = sc.parallelize(data)\n\n\n## Plot the data\n\n# Take (at random) 500 data from dataRDD\ndata500 = dataRDD.takeSample(withReplacement=False, num=500)\n\n# Note that you are getting a list of arrays, convert it to numpy array (use np.array() method)\ndata500 = np.array(data500)\n\n# Plot the data\nfig = plt.figure()\nplt.plot(data500[:,0],data500[:,1],'.')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["##### 1. Initial setting of the parameters\n\n**Exercise**: Complete the following cell to set the initial values of the GMM parameters. Note that the cell is designed to use the MultivariateGaussian object to define the mean and covariance matrix of each gaussian.\n\nUseful functions:\n* [np.random.rand()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.rand.html)\n* [np.random.randn()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randn.html)\n* [np.eye()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html)\n* [MultivariateGaussian](https://spark.apache.org/docs/2.1.2/api/python/pyspark.mllib.html#pyspark.mllib.stat.MultivariateGaussian)"],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\nfrom pyspark.mllib.linalg import Vectors, DenseMatrix\nfrom pyspark.mllib.stat import MultivariateGaussian\n\n# Set a seed to get similar results\nnp.random.seed(1)\n\ndef initializeGMM(dataRDD, K):\n    \"\"\"Initialize the  GMM model parameters (gaussians, weights) and compute some useful parameter\n    Args:\n      dataRDD: RDD with the data used to learn the model, each element is numpy array \n      K: number of gaussians of the GMM\n    Returns: \n      gaussians: list with the gaussians of the GMM. Each gaussian is an object with the mean and coviariance of the gaussian.\n      weights: weights for each Gaussian distribution in the GMM \n      D: data dimension\n      N: number of data used to learn the model\n    \"\"\"\n    # Compute data dimension\n    D =  len(dataRDD.first())\n    # Precompute the number of data in dataRDD\n    N =  dataRDD.count()\n\n    # Initialize the weigths of each gaussian with a uniform value between 0 and 1 (use np.random.rand). Remember $0<\\pi_k<1$ and $\\sum_k         \\pi_k =1$. weights has to be a vector of dimension K\n    weights =  np.random.rand(K)\n    weights =  weights/np.double(np.sum(weights))\n\n    \n    gaussians = []\n    for k in range(K):\n      # For each gaussian, define the gaussian parameters (mean and covariances)\n\n      # Initialize the means of each gaussian with a normal value with zero mean and unitary standard deviation (use np.random.randn)\n      # Note that each mean is a vector of length D\n      mu=  np.random.randn(D)\n\n      # Initialize the covariances with the identity matrix (use np.eye). The covariance matrix has to be a matrix of dimension DxD.\n      sigmaValues=  np.eye(D) \n\n      # Initialize the MultivariateGaussian object with the mean and covariace values (see MultivariateGaussian)\n      myGauss=  MultivariateGaussian(Vectors.dense(mu),DenseMatrix(D,D,sigmaValues.reshape(-1),False))\n\n      # Add the myGauss object to a list with the gaussians\n      gaussians.append(myGauss)\n    \n    return gaussians, weights, D, N\n\ngaussians, weights, D, N = initializeGMM(dataRDD, 4)\nprint 'weights: ', weights\nprint 'gaussians: ', gaussians \nprint 'Number of data: ', N"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nweights:  [  2.89640161e-01   5.00297106e-01   7.94383512e-05   2.09983296e-01]\ngaussians:  [MultivariateGaussian(mu=DenseVector([-0.5282, -1.073]), sigma=DenseMatrix(2, 2, [1.0, 0.0, 0.0, 1.0], False)), MultivariateGaussian(mu=DenseVector([0.8654, -2.3015]), sigma=DenseMatrix(2, 2, [1.0, 0.0, 0.0, 1.0], False)), MultivariateGaussian(mu=DenseVector([1.7448, -0.7612]), sigma=DenseMatrix(2, 2, [1.0, 0.0, 0.0, 1.0], False)), MultivariateGaussian(mu=DenseVector([0.319, -0.2494]), sigma=DenseMatrix(2, 2, [1.0, 0.0, 0.0, 1.0], False))]\nNumber of data:  2000\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["##### 2. E-step: Compute the responsabilities \n\n**Exercise**: Complete the following cell to desing a function able to compute the responsability vector associated to each data of the RDD (note that, for each data, the responsability vector has as many elements as number of centroids). Remember that the responsability of a data over the centroid *k* is given by:\n\n$$\\gamma(z_{k}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k) }{\\sum_j^K \\pi_j \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_j,\\mathbf{\\Sigma}_j) }$$\n\nto compute the values of likelihood of each gaussian over the data **x** you can use the function computeLikelihoodPerGauss() designed in Section 1.3."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\ndef computeResponsabilities(gaussians, weights, x):\n  \n    \"\"\"Compute the responsability vector of the data x given the model parameters (gaussians, weights)\n    Args:\n      gaussians: list with the gaussians of the GMM. Each gaussian is an object with the mean and coviariance of the gaussian.\n      weights: weights for each Gaussian distribution in the GMM \n      x: numpy array with a data\n    Returns: \n      Resposabilities: numpy array with the responsabilities associated to data x\n    \"\"\"\n    N = computeLikelihoodPerGauss(gaussians, x)      \n    return np.multiply(weights,N)/np.sum(np.multiply(weights,N))\n\n# Check the function over a data\nx = dataRDD.first()\nResponsabilities = computeResponsabilities(gaussians, weights, x)\nprint 'Responsability vector over a data:', Responsabilities"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nResponsability vector over a data: [  8.99157388e-05   3.26233611e-05   8.22579735e-03   9.91651664e-01]\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["**Exercise**: Use the previous function to compute the responsibilities of each data in dataRDD. Create a new RDD where each element has to be a tuple: (data, responsabilities)."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\n# Compute the responsabilities over dataRDD\nRDDResponsabilities = dataRDD.map(lambda x: (x,computeResponsabilities(gaussians, weights, x)))\n\nResponsabilities = RDDResponsabilities.take(5)   \nprint 'Responsabilities over some data: ', Responsabilities                                                                "],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nResponsabilities over some data:  [(array([ 5.3315865 ,  5.43925373]), array([  8.99157388e-05,   3.26233611e-05,   8.22579735e-03,\n         9.91651664e-01])), (array([ 4.71527897,  5.38214662]), array([  1.59605508e-04,   2.63152190e-05,   3.53405794e-03,\n         9.96280021e-01])), (array([ 2.45459971,  4.92132043]), array([  1.58676577e-03,   1.97393996e-05,   1.78536898e-04,\n         9.98214958e-01])), (array([ 3.99161615,  3.42711278]), array([  1.47115864e-03,   9.77171617e-04,   3.41838680e-03,\n         9.94133283e-01])), (array([ 4.62133597,  5.2750491 ]), array([  1.88808817e-04,   3.11507083e-05,   3.26596973e-03,\n         9.96514071e-01]))]\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["##### 3. M-step: Reestimate the new values of the parameters\n**Exercise**: Complete the following cell to desing a function able to update the values of the parameters. Remember that the parameter values are updated using the following expressions:\n\n$$ \\mathbf{\\mu}^{new}_k = \\frac{1}{N_k} \\sum_i^N  \\gamma(z^{(i)}_k) \\mathbf{x}^{(i)} \\quad \\quad (1)$$\n$$ \\mathbf{\\Sigma}^{new}_k = \\frac{1}{N_k} \\sum_i^N  \\gamma(z^{(i)}_k) (\\mathbf{x}^{(i)} -\\mathbf{\\mu}^{new}_k)(\\mathbf{x}^{(i)} -\\mathbf{\\mu}^{new}_k)^T \\quad \\quad (2)$$\n$$ \\pi^{new}_k = \\frac{N_k}{N} \\quad \\quad (3)$$\nwhere\n$$ N_k = \\sum_i^N  \\gamma(z^{(i)}_k) \\quad \\quad (4)$$\n\nKeep in mind that these operations require calculating sums over all data or over some transformations of the data, remember that you can do it efficiently in Spark using correctly the operator reduce() or combining map() transformations with a final reduce()."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\ndef updateParameters(RDDResponsabilities):\n    \"\"\"Reestimate the GMM parameters (gaussians, weights)\n    Args:\n      RDDResponsabilities: RDD where each element is a tuple (x, responsability). x is a numpy array with a data and responsability a numpy                            array with the responsabilities associated to x.\n    Returns: \n      gaussians: list with the gaussians of the GMM. Each gaussian is an object with the mean and coviariance of the gaussian.\n      weights: weights for each Gaussian distribution in the GMM \n    \"\"\"\n\n    # Compute Nk, a vector with K values with the sum of the responsabilities associated to each gaussian (eq. (4))\n    Nk = RDDResponsabilities.map(lambda x: x[1]).reduce(lambda accum, x: accum+x)\n\n    # Update the weigth values (eq. (3))\n    weights_new = Nk/RDDResponsabilities.count()\n\n    gaussians_new=[]\n    for k in range(K): \n      # For each gaussian, we update its mean and covariance\n      # \\mu_k -> eq. (1)\n      mu_k = np.divide(RDDResponsabilities.map(lambda x: np.dot(x[1].reshape(-1,1),x[0].reshape(1,-1))).reduce(lambda accum, x: accum+x)[k,:],Nk[k])\n      # \\sigma_k -> eq (2)\n      sigma_k = np.divide(RDDResponsabilities.map(lambda x:  np.multiply(np.dot((x[0]-mu_k).reshape(-1,1),(x[0]-mu_k).reshape(1,-1)),x[1][k])).reduce(lambda accum,x: accum+x),Nk[k])\n      \n      # Save these values into a MultivariateGaussian object (see MultivariateGaussian)\n      myGauss= MultivariateGaussian(Vectors.dense(mu_k),DenseMatrix(D,D,sigma_k.reshape(-1),False))\n\n      # Add the myGauss object to a list with the gaussians\n      gaussians_new.append(myGauss)\n\n    return weights_new, gaussians_new\n\nweights, gaussians= updateParameters(RDDResponsabilities)\nprint 'weights: ', weights\nprint 'gaussians: ', gaussians "],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nweights:  [ 0.32204981  0.29330512  0.00188603  0.38275904]\ngaussians:  [MultivariateGaussian(mu=DenseVector([-4.2057, -1.1911]), sigma=DenseMatrix(2, 2, [0.9647, -0.2072, -0.2072, 13.8612], False)), MultivariateGaussian(mu=DenseVector([2.9161, -4.1861]), sigma=DenseMatrix(2, 2, [7.621, 0.694, 0.694, 1.0969], False)), MultivariateGaussian(mu=DenseVector([5.2418, 3.3657]), sigma=DenseMatrix(2, 2, [0.813, 0.0439, 0.0439, 1.6005], False)), MultivariateGaussian(mu=DenseVector([1.3061, 4.0293]), sigma=DenseMatrix(2, 2, [14.3911, -0.493, -0.493, 0.9979], False))]\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["##### 4. Evaluate the log likelihood\n**Exercise**: Complete the following function to be able to evaluate the log-likelihood over all training data:\n\n$$ \\sum_i^{N} \\log  p(\\mathbf{x}^{(i)}|\\mathbf{\\theta}) = \\sum_i^{N} \\log \\left( \\sum_k^K \\pi_k \\mathcal{N}(\\mathbf{x}^{(i)}|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k) \\right)$$\n\nYou can use the function computeGMMLogLikelihood(), implemented in Section 1.3, which computes the log-likelihood over a given data."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\ndef computeDataLogLikelihood(dataRDD, gaussians, weights):\n    \"\"\"Compute the log likelihood of a learned GMM over all training data\n    Args:\n       dataRDD: RDD with the data used to learn the model, each element is numpy array \n       gaussians: list with the gaussians of the GMM. Each gaussian is an object with the mean and coviariance of the gaussian.\n       weights: weights for each Gaussian distribution in the GMM \n    Returns: \n       logLikelihood: log likelihood of the GMM, defined by gaussians, weights, computed over all training data in RDDdata\n    \"\"\"\n    \n    logLikelihood = dataRDD.map(lambda x: computeGMMLogLikelihood(gaussians, weights,x)).reduce(lambda accum,x: accum+x)\n    return logLikelihood\n  \nlogLikelihood = computeDataLogLikelihood(dataRDD, gaussians, weights)\nprint 'log-likelihood: ', logLikelihood"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nlog-likelihood:  -9688.59302486\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["##### 5. Implementing EM algorithm\n**Exercise**: Combine all steps of the EM algorithm to learn the GMM parameters (set the number of Gaussians to 4). Run the algorithm for 20 iterations. Analyze the evolution of the log-likelihood in each iteration and compare the parameters of the model with their real values (the values used to generate the data)."],"metadata":{}},{"cell_type":"code","source":["#Complete the #FILL IN# gaps\n\n# Set a seed to get similar results\nnp.random.seed(0)\n\n# 1. Initialize the GMM model (K=4)\ngaussians, weights, D, N =  initializeGMM(dataRDD, 4)\n\nfor iter in range(20):\n  print 'Iteration number: ', iter\n  # 2. E-step: Compute the responsabilities \n  RDDResponsabilities =  dataRDD.map(lambda x: (x,computeResponsabilities(gaussians, weights, x)))\n\n  # 3. M-step: Reestimate the new values of the parameters:\n  weights, gaussians=  updateParameters(RDDResponsabilities)\n  \n  print 'weights: ', weights\n  print 'gaussians: ', gaussians \n\n  # 4. Evaluate the log likelihood\n  logLikelihood =  computeDataLogLikelihood(dataRDD, gaussians, weights)\n  print 'log-likelihood: ', logLikelihood"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["**_The answer should be_:**\n<pre><code>\nIteration number:  0\nweights:  [ 0.2804534   0.06909013  0.24722014  0.40323633]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.7741, -3.5854]), sigma=DenseMatrix(2, 2, [3.0252, 0.8949, 0.8949, 4.6096], False)), MultivariateGaussian(mu=DenseVector([0.5922, -0.6555]), sigma=DenseMatrix(2, 2, [15.1184, 13.7086, 13.7086, 15.0101], False)), MultivariateGaussian(mu=DenseVector([-4.0299, -2.7157]), sigma=DenseMatrix(2, 2, [1.85, 0.9253, 0.9253, 8.2056], False)), MultivariateGaussian(mu=DenseVector([-0.2292, 4.1152]), sigma=DenseMatrix(2, 2, [16.2334, 0.2441, 0.2441, 0.8864], False))]\nlog-likelihood:  -9806.6802662\nIteration number:  1\nweights:  [ 0.25418893  0.05374757  0.25378754  0.43827596]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9804, -3.9982]), sigma=DenseMatrix(2, 2, [1.1476, 0.0842, 0.0842, 1.5736], False)), MultivariateGaussian(mu=DenseVector([0.81, -0.0584]), sigma=DenseMatrix(2, 2, [15.5186, 14.7169, 14.7169, 15.4414], False)), MultivariateGaussian(mu=DenseVector([-4.0534, -3.2508]), sigma=DenseMatrix(2, 2, [0.8707, 0.1165, 0.1165, 5.7119], False)), MultivariateGaussian(mu=DenseVector([-0.0364, 4.0651]), sigma=DenseMatrix(2, 2, [16.8197, -0.1019, -0.1019, 0.7651], False))]\nlog-likelihood:  -9401.67057126\nIteration number:  2\nweights:  [ 0.25005022  0.04474165  0.24963264  0.45557549]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.98, -4.0947]), sigma=DenseMatrix(2, 2, [1.1009, 0.0277, 0.0277, 0.9662], False)), MultivariateGaussian(mu=DenseVector([1.3248, 0.4928]), sigma=DenseMatrix(2, 2, [14.4257, 13.8426, 13.8426, 14.6366], False)), MultivariateGaussian(mu=DenseVector([-4.0615, -3.5962]), sigma=DenseMatrix(2, 2, [0.844, -0.0233, -0.0233, 3.6344], False)), MultivariateGaussian(mu=DenseVector([-0.0657, 4.0318]), sigma=DenseMatrix(2, 2, [16.8153, -0.1564, -0.1564, 0.7939], False))]\nlog-likelihood:  -9304.70913122\nIteration number:  3\nweights:  [ 0.25003463  0.03649431  0.24392499  0.46954608]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9794, -4.0957]), sigma=DenseMatrix(2, 2, [1.1026, 0.0267, 0.0267, 0.9613], False)), MultivariateGaussian(mu=DenseVector([1.9458, 1.1532]), sigma=DenseMatrix(2, 2, [12.6475, 12.384, 12.384, 13.4229], False)), MultivariateGaussian(mu=DenseVector([-4.0487, -3.9087]), sigma=DenseMatrix(2, 2, [0.8822, -0.0199, -0.0199, 1.789], False)), MultivariateGaussian(mu=DenseVector([-0.1442, 3.9882]), sigma=DenseMatrix(2, 2, [16.8099, -0.0467, -0.0467, 0.8487], False))]\nlog-likelihood:  -9194.63800768\nIteration number:  4\nweights:  [ 0.25005594  0.02981469  0.24398104  0.47614833]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0956]), sigma=DenseMatrix(2, 2, [1.1033, 0.0267, 0.0267, 0.9615], False)), MultivariateGaussian(mu=DenseVector([2.7567, 2.0317]), sigma=DenseMatrix(2, 2, [9.2888, 9.3189, 9.3189, 10.5701], False)), MultivariateGaussian(mu=DenseVector([-4.032, -4.0369]), sigma=DenseMatrix(2, 2, [0.922, 0.0056, 0.0056, 1.0646], False)), MultivariateGaussian(mu=DenseVector([-0.1739, 3.9603]), sigma=DenseMatrix(2, 2, [16.8157, 0.016, 0.016, 0.8947], False))]\nlog-likelihood:  -9151.07145816\nIteration number:  5\nweights:  [ 0.25006057  0.02696284  0.24714114  0.47583545]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0955]), sigma=DenseMatrix(2, 2, [1.1031, 0.0266, 0.0266, 0.9617], False)), MultivariateGaussian(mu=DenseVector([3.5409, 2.9274]), sigma=DenseMatrix(2, 2, [4.9772, 5.0658, 5.0658, 6.2619], False)), MultivariateGaussian(mu=DenseVector([-4.0202, -4.0436]), sigma=DenseMatrix(2, 2, [0.9457, 0.0129, 0.0129, 1.0338], False)), MultivariateGaussian(mu=DenseVector([-0.1813, 3.9546]), sigma=DenseMatrix(2, 2, [16.8125, 0.0034, 0.0034, 0.9044], False))]\nlog-likelihood:  -9135.11157631\nIteration number:  6\nweights:  [ 0.25006874  0.03078927  0.24944132  0.46970067]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9791, -4.0955]), sigma=DenseMatrix(2, 2, [1.1034, 0.0266, 0.0266, 0.9617], False)), MultivariateGaussian(mu=DenseVector([4.1338, 3.6532]), sigma=DenseMatrix(2, 2, [1.2035, 1.0698, 1.0698, 1.9046], False)), MultivariateGaussian(mu=DenseVector([-4.0077, -4.041]), sigma=DenseMatrix(2, 2, [0.9666, 0.0234, 0.0234, 1.038], False)), MultivariateGaussian(mu=DenseVector([-0.2384, 3.9533]), sigma=DenseMatrix(2, 2, [16.7735, -0.0075, -0.0075, 0.9058], False))]\nlog-likelihood:  -9078.8774396\nIteration number:  7\nweights:  [ 0.25006601  0.05580298  0.24993436  0.44419665]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0954]), sigma=DenseMatrix(2, 2, [1.103, 0.0266, 0.0266, 0.9618], False)), MultivariateGaussian(mu=DenseVector([4.2261, 3.8234]), sigma=DenseMatrix(2, 2, [0.4869, 0.2813, 0.2813, 0.9716], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0394]), sigma=DenseMatrix(2, 2, [0.9747, 0.0269, 0.0269, 1.0389], False)), MultivariateGaussian(mu=DenseVector([-0.4943, 3.9568]), sigma=DenseMatrix(2, 2, [16.5105, 0.0003, 0.0003, 0.914], False))]\nlog-likelihood:  -8989.78323055\nIteration number:  8\nweights:  [ 0.25006248  0.10004935  0.24993819  0.39994999]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0954]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9618], False)), MultivariateGaussian(mu=DenseVector([4.2216, 3.8723]), sigma=DenseMatrix(2, 2, [0.3807, 0.1329, 0.1329, 0.8231], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0394]), sigma=DenseMatrix(2, 2, [0.9748, 0.0268, 0.0268, 1.0389], False)), MultivariateGaussian(mu=DenseVector([-1.0154, 3.9593]), sigma=DenseMatrix(2, 2, [15.5837, 0.0195, 0.0195, 0.9454], False))]\nlog-likelihood:  -8901.96412127\nIteration number:  9\nweights:  [ 0.25006217  0.14269237  0.2499381   0.35730736]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9793, -4.0954]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9618], False)), MultivariateGaussian(mu=DenseVector([4.2031, 3.9037]), sigma=DenseMatrix(2, 2, [0.3906, 0.0649, 0.0649, 0.807], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0394]), sigma=DenseMatrix(2, 2, [0.9748, 0.0268, 0.0268, 1.0389], False)), MultivariateGaussian(mu=DenseVector([-1.6331, 3.9571]), sigma=DenseMatrix(2, 2, [13.8167, 0.0201, 0.0201, 0.9674], False))]\nlog-likelihood:  -8838.30247634\nIteration number:  10\nweights:  [ 0.25006194  0.1751064   0.2499379   0.32489377]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9793, -4.0954]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9618], False)), MultivariateGaussian(mu=DenseVector([4.1847, 3.9211]), sigma=DenseMatrix(2, 2, [0.4509, 0.0297, 0.0297, 0.8305], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0394]), sigma=DenseMatrix(2, 2, [0.9749, 0.0268, 0.0268, 1.0389], False)), MultivariateGaussian(mu=DenseVector([-2.2054, 3.9531]), sigma=DenseMatrix(2, 2, [11.5134, 0.0081, 0.0081, 0.9712], False))]\nlog-likelihood:  -8788.3513846\nIteration number:  11\nweights:  [ 0.25006138  0.20053063  0.24993778  0.29947021]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9793, -4.0955]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9618], False)), MultivariateGaussian(mu=DenseVector([4.1711, 3.9298]), sigma=DenseMatrix(2, 2, [0.543, 0.0161, 0.0161, 0.851], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0394]), sigma=DenseMatrix(2, 2, [0.9749, 0.0268, 0.0268, 1.0388], False)), MultivariateGaussian(mu=DenseVector([-2.7388, 3.95]), sigma=DenseMatrix(2, 2, [8.7564, -0.0061, -0.0061, 0.9697], False))]\nlog-likelihood:  -8729.58945124\nIteration number:  12\nweights:  [ 0.25006078  0.22336416  0.24993765  0.27663741]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0955]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9617], False)), MultivariateGaussian(mu=DenseVector([4.1562, 3.9335]), sigma=DenseMatrix(2, 2, [0.6705, 0.008, 0.008, 0.8617], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0394]), sigma=DenseMatrix(2, 2, [0.9749, 0.0268, 0.0268, 1.0388], False)), MultivariateGaussian(mu=DenseVector([-3.2971, 3.9486]), sigma=DenseMatrix(2, 2, [5.245, -0.012, -0.012, 0.971], False))]\nlog-likelihood:  -8622.91721616\nIteration number:  13\nweights:  [ 0.2500603   0.24209714  0.24993739  0.25790517]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0955]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9617], False)), MultivariateGaussian(mu=DenseVector([4.0917, 3.9365]), sigma=DenseMatrix(2, 2, [0.7869, -0.0023, -0.0023, 0.8671], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0394]), sigma=DenseMatrix(2, 2, [0.9749, 0.0268, 0.0268, 1.0388], False)), MultivariateGaussian(mu=DenseVector([-3.7779, 3.9469]), sigma=DenseMatrix(2, 2, [2.0999, -0.0183, -0.0183, 0.974], False))]\nlog-likelihood:  -8454.8582518\nIteration number:  14\nweights:  [ 0.25006     0.24929257  0.24993703  0.2507104 ]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0955]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9617], False)), MultivariateGaussian(mu=DenseVector([4.0398, 3.9394]), sigma=DenseMatrix(2, 2, [0.8693, -0.0082, -0.0082, 0.8636], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0395]), sigma=DenseMatrix(2, 2, [0.9749, 0.0268, 0.0268, 1.0388], False)), MultivariateGaussian(mu=DenseVector([-3.9521, 3.9443]), sigma=DenseMatrix(2, 2, [1.0581, -0.0338, -0.0338, 0.9806], False))]\nlog-likelihood:  -8394.19454426\nIteration number:  15\nweights:  [ 0.25006025  0.24999849  0.24993684  0.25000442]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0955]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9617], False)), MultivariateGaussian(mu=DenseVector([4.0311, 3.9398]), sigma=DenseMatrix(2, 2, [0.8939, -0.0094, -0.0094, 0.8616], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0395]), sigma=DenseMatrix(2, 2, [0.9749, 0.0268, 0.0268, 1.0388], False)), MultivariateGaussian(mu=DenseVector([-3.966, 3.9439]), sigma=DenseMatrix(2, 2, [0.9926, -0.0359, -0.0359, 0.9829], False))]\nlog-likelihood:  -8393.5152757\nIteration number:  16\nweights:  [ 0.25006037  0.25000136  0.24993682  0.25000145]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0955]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9617], False)), MultivariateGaussian(mu=DenseVector([4.031, 3.9398]), sigma=DenseMatrix(2, 2, [0.894, -0.0094, -0.0094, 0.8616], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0395]), sigma=DenseMatrix(2, 2, [0.9749, 0.0268, 0.0268, 1.0388], False)), MultivariateGaussian(mu=DenseVector([-3.966, 3.9439]), sigma=DenseMatrix(2, 2, [0.9923, -0.0359, -0.0359, 0.9829], False))]\nlog-likelihood:  -8393.51526348\nIteration number:  17\nweights:  [ 0.25006037  0.25000137  0.24993682  0.25000144]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0955]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9617], False)), MultivariateGaussian(mu=DenseVector([4.031, 3.9398]), sigma=DenseMatrix(2, 2, [0.894, -0.0094, -0.0094, 0.8616], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0395]), sigma=DenseMatrix(2, 2, [0.9749, 0.0268, 0.0268, 1.0388], False)), MultivariateGaussian(mu=DenseVector([-3.966, 3.9439]), sigma=DenseMatrix(2, 2, [0.9923, -0.0359, -0.0359, 0.9829], False))]\nlog-likelihood:  -8393.51526348\nIteration number:  18\nweights:  [ 0.25006037  0.25000137  0.24993682  0.25000144]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0955]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9617], False)), MultivariateGaussian(mu=DenseVector([4.031, 3.9398]), sigma=DenseMatrix(2, 2, [0.894, -0.0094, -0.0094, 0.8616], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0395]), sigma=DenseMatrix(2, 2, [0.9749, 0.0268, 0.0268, 1.0388], False)), MultivariateGaussian(mu=DenseVector([-3.966, 3.9439]), sigma=DenseMatrix(2, 2, [0.9923, -0.0359, -0.0359, 0.9829], False))]\nlog-likelihood:  -8393.51526348\nIteration number:  19\nweights:  [ 0.25006037  0.25000137  0.24993682  0.25000144]\ngaussians:  [MultivariateGaussian(mu=DenseVector([3.9792, -4.0955]), sigma=DenseMatrix(2, 2, [1.1028, 0.0266, 0.0266, 0.9617], False)), MultivariateGaussian(mu=DenseVector([4.031, 3.9398]), sigma=DenseMatrix(2, 2, [0.894, -0.0094, -0.0094, 0.8616], False)), MultivariateGaussian(mu=DenseVector([-4.0035, -4.0395]), sigma=DenseMatrix(2, 2, [0.9749, 0.0268, 0.0268, 1.0388], False)), MultivariateGaussian(mu=DenseVector([-3.966, 3.9439]), sigma=DenseMatrix(2, 2, [0.9923, -0.0359, -0.0359, 0.9829], False))]\nlog-likelihood:  -8393.51526348\n</code></pre>"],"metadata":{}},{"cell_type":"markdown","source":["## Conclusion\n\nWe created the sample using four means, (4,4), (4,-4), (-4,4) and (-4,-4) with convariance matrix as follow :\n\n| 1 | 0 |\n|---|---|\n| 0 | 1 |\n\nThe final parameters are (4.031, 3.939),(3.9792, -4.0955),(-3.966, 3.9439) and (-4.0035, -4.0395) which are very close to the original and the same with the covariance matrix, all of them are very similar to the original ones.\n\nThe logLiklihood started in -9806.6802662 and in the end is -8393.51526348, which is much better."],"metadata":{}}],"metadata":{"name":"GMM_lab_session_students","notebookId":3079637607622238},"nbformat":4,"nbformat_minor":0}
